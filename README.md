# Intramove: Automatizaci√≥n del flujo de datos operativos con AWS Glue DataBrew
## Beneficios estrat√©gicos de implementar un pipeline en Intramove
En este proyecto dise√±√© y puse en marcha un flujo automatizado de trabajo con datos, tambi√©n conocido como 'pipeline'. De forma sencilla, se trata de una secuencia de pasos que permite recoger datos brutos, limpiarlos, organizarlos y dejarlos listos para ser analizados sin tener que hacerlo manualmente cada vez.

Este proceso, que funciona en la nube, simula lo que una empresa real de log√≠stica como Intramove har√≠a a diario con sus datos de ventas y entregas: recibir archivos, revisarlos, corregirlos si es necesario, calcular m√©tricas importantes y guardarlos preparados para sacar conclusiones y tomar decisiones.

A continuaci√≥n, resumo los beneficios concretos que este tipo de sistema puede aportar a una empresa de este sector:

| üß© **Beneficio clave** | üìà **Impacto empresarial** |
|-------------------------|-----------------------------|
| Optimizaci√≥n del control log√≠stico diario. Automatiza la limpieza y transformaci√≥n de los datos de entregas y ventas. | Seguimiento preciso del rendimiento log√≠stico y de la demanda. |
| Reducci√≥n de tareas manuales repetitivas. Se programa para ejecutarse autom√°ticamente cuando se suben nuevos datos. | Menos tareas manuales, m√°s tiempo para an√°lisis estrat√©gico. |
| Mejora de la calidad y la fiabilidad del dato gracias a procesos de limpieza y transformaci√≥n estandarizados, lo que minimiza los errores humanos. | Decisiones basadas en datos √≠ntegros, coherentes y actualizados. |
| Datos siempre listos para el equipo de an√°lisis. Los archivos limpios se almacenan autom√°ticamente en S3 y pueden ser consumidos desde Power BI, Tableau o Python. | Mayor autonom√≠a del √°rea de an√°lisis y reportes m√°s veloces. |
| Ahorro de costes con tecnolog√≠a cloud escalable. AWS Glue DataBrew y S3 ofrecen un modelo de pago por uso con capa gratuita, ideal para empresas que quieren escalar sin inversiones iniciales elevadas. | Soluci√≥n profesional sin necesidad de infraestructura propia ni desarrolladores. |
| Integraci√≥n con toda la arquitectura AWS. Se apoya en servicios como: S3 (almacenamiento), Glue (cat√°logo), IAM (seguridad). Es f√°cil integrarlo luego con Redshift, Athena o SageMaker. | Base s√≥lida para una futura anal√≠tica avanzada o inteligencia artificial log√≠stica. |

## Habilidades demostradas en este proyecto
- Conciencia del impacto en negocio: Foco constante en la automatizaci√≥n y el ahorro de tiempo, reduciendo tareas manuales repetitivas mediante un flujo de trabajo visual, reproducible y escalable.¬†

- Fundamentos de ingenier√≠a de datos y flujo ETL: Aplicaci√≥n de un proceso ETL completo en entorno cloud. Comprensi√≥n y dise√±o del flujo: ingesta (Extract), limpieza y transformaci√≥n (Transform), y exportaci√≥n estructurada para an√°lisis (Load).

- Automatizaci√≥n y eficiencia de procesos: Configuraci√≥n de flujos que simulan automatizaciones reales: recepci√≥n diaria de archivos brutos, ejecuci√≥n de trabajos de transformaci√≥n y generaci√≥n de salidas listas para an√°lisis.

- Calidad, validaci√≥n y gobernanza del dato: Aplicaci√≥n de buenas pr√°cticas en la limpieza y validaci√≥n: detecci√≥n de inconsistencias, eliminaci√≥n de duplicados y transformaciones estandarizadas, asegurando la integridad del dato.‚Äã‚Äã‚Äã‚Äã‚Äã

- Uso de tecnolog√≠as cloud y herramientas visuales: Dise√±o e implementaci√≥n de un pipeline completo en AWS, utilizando: Amazon S3 para almacenamiento estructurado, AWS Glue para catalogaci√≥n automatizada, AWS Glue DataBrew para limpieza y transformaci√≥n.¬†

## Prop√≥sito y contexto del proyecto

Este proyecto tiene como objetivo dise√±ar y poner en marcha un flujo automatizado de tratamiento de datos en la nube para IntraMove, una empresa log√≠stica ficticia que gestiona entregas y ventas diarias.

En su actividad habitual, esta empresa recibe archivos con informaci√≥n sobre pedidos y entregas, normalmente sin procesar, con formatos inconsistentes o errores. Procesar estos datos manualmente supone un consumo elevado de tiempo y recursos, adem√°s de un mayor riesgo de errores humanos.

Para abordar este desaf√≠o, se construy√≥ un pipeline de datos visual en AWS que permite:

- Recoger los archivos brutos autom√°ticamente desde almacenamiento en la nube.

- Aplicar transformaciones y limpieza estructurada: c√°lculo de m√©tricas, correcci√≥n de formatos, creaci√≥n de nuevas variables.

- Generar archivos limpios y organizados, listos para an√°lisis con herramientas de BI o lenguajes como Python.

Esta soluci√≥n est√° orientada a la mejora operativa, permitiendo a IntraMove disponer de datos fiables y actualizados sin depender de tareas manuales, y ofreciendo una base s√≥lida para futuros procesos de anal√≠tica avanzada.

 ## Desarrollo t√©cnico del pipeline de datos
 A continuaci√≥n, se detallan las fases desarrolladas durante el proyecto, desde la preparaci√≥n del entorno hasta la automatizaci√≥n del flujo de datos.
 ### Fases del an√°lisis
1. **Preparaci√≥n de los datos y entorno cloud**


Simulaci√≥n de archivos


Se generaron dos archivos .csv representando datos operativos t√≠picos de una empresa log√≠stica:
- ventas.csv: contiene informaci√≥n de las ventas realizadas, incluyendo columnas como fecha de venta, producto, cantidad, precio unitario, cliente y almac√©n de origen.
- logistica.csv: recoge los detalles log√≠sticos asociados a esas ventas, incluyendo la fecha de env√≠o y entrega, el estado de la entrega y el nombre del repartidor.

Subida a Amazon S3


Despu√©s, se cre√≥ un bucket en Amazon S3, el sistema de almacenamiento en la nube de AWS, para alojar los archivos y organizar el flujo de trabajo:

- Carpeta /raw/: para almacenar los datos en bruto sin transformar.

- Carpeta /processed/: para guardar los archivos finales ya limpios y listos para an√°lisis.

Esta estructura permite separar claramente los datos originales de los resultados del pipeline, siguiendo buenas pr√°cticas de almacenamiento en entornos cloud.

![1 (2)](https://github.com/user-attachments/assets/9fa637ac-3d54-4521-97c3-e255a5c3b38a)


**2. Creaci√≥n del cat√°logo de datos con AWS Glue Crawler**

Una vez almacenados los archivos en Amazon S3, el siguiente paso fue automatizar la detecci√≥n de su estructura para poder trabajar con ellos como si fueran tablas en una base de datos. Para ello, se utiliz√≥ AWS Glue Crawler, una herramienta que permite escanear datos en S3 y catalogarlos autom√°ticamente.


Configuraci√≥n del crawler


Se cre√≥ un crawler que apunta a la ruta /raw/ del bucket, donde se encuentran los archivos originales. Este crawler:

- Detecta de forma autom√°tica las columnas, tipos de datos y formato de los archivos CSV.

- Genera tablas estructuradas en el Glue Data Catalog, el cat√°logo central de datos de AWS.

- Permite que herramientas como AWS DataBrew o Amazon Athena trabajen con los datos sin necesidad de prepararlos manualmente.

‚Äã

Resultado: tablas estructuradas listas para usar


‚Äã
Tras ejecutar el crawler, se generaron dos tablas en una base de datos Glue llamada intramove_db:

- ventas_csv: representa la tabla estructurada del archivo de ventas.

- logistica_csv: representa la tabla de entregas log√≠sticas.

Gracias a este paso, los archivos ya no son simplemente documentos sueltos en un sistema de archivos, sino datasets estructurados y gestionables desde el ecosistema cloud de AWS, preparados para ser transformados y analizados.

‚Äã![2](https://github.com/user-attachments/assets/934a1688-7749-483f-adb6-55e2d0cbaa2c)


**3. Limpieza y transformaci√≥n visual con AWS Glue DataBrew**

Una vez catalogados los datos, se procedi√≥ a la fase central del pipeline: la limpieza, validaci√≥n y transformaci√≥n visual de los datasets usando AWS Glue DataBrew. Esta herramienta permite aplicar reglas de calidad y transformaciones complejas de forma visual, sin necesidad de escribir c√≥digo, ideal para flujos ETL accesibles y replicables.

Proyecto 1: Transformaci√≥n de datos de ventas
‚Äã
Se cre√≥ un proyecto en DataBrew basado en la tabla ventas_csv. A partir de esta fuente, se aplicaron varias transformaciones clave para preparar los datos para an√°lisis comercial:

- Revisi√≥n y correcci√≥n de tipos de datos (fechas, num√©ricos, texto).

- Creaci√≥n de la columna total_venta: resultado de multiplicar la cantidad por el precio unitario.

- Extracci√≥n de componentes de la fecha (a√±o, mes) para permitir an√°lisis temporales.

- Verificaci√≥n de calidad: detecci√≥n de duplicados y validaci√≥n de consistencia en las columnas clave.

Estas transformaciones permiten construir KPIs como ingresos mensuales, ventas por producto o rendimiento por almac√©n.

Proyecto 2: Transformaci√≥n de datos log√≠sticos
‚Äã
En paralelo, se cre√≥ un segundo proyecto sobre la tabla logistica_csv, centrado en el an√°lisis de la eficiencia de las entregas.¬†Transformaciones aplicadas:

- C√°lculo del tiempo de entrega (tiempo_entrega_dias) mediante la diferencia entre fecha_entrega y fecha_envio.

- Creaci√≥n de la columna entrega_clasificada, que indica si la entrega fue ‚ÄúA tiempo‚Äù o ‚ÄúRetrasada‚Äù seg√∫n el tiempo calculado.

- Verificaci√≥n de formatos y limpieza de inconsistencias.

Gracias a estas transformaciones, se pueden analizar m√©tricas como la puntualidad log√≠stica o la eficiencia por repartidor o ruta.

![3](https://github.com/user-attachments/assets/7495db43-d45b-43b1-ac56-10beadcf03a0)


**4: Exportaci√≥n del dataset limpio a S3**

Una vez aplicadas todas las transformaciones en AWS Glue DataBrew, se configuraron trabajos (jobs) para ejecutar las recetas completas sobre los datasets originales y exportar los resultados transformados al almacenamiento en la nube.
‚Äã
Creaci√≥n de los trabajos en DataBrew
‚Äã
Se crearon dos trabajos independientes, uno para cada proyecto:

- Job de ventas: aplic√≥ todas las transformaciones del proyecto ventas_csv y gener√≥ un archivo limpio con m√©tricas clave como total_venta, a√±o, mes.

- Job de log√≠stica: ejecut√≥ la receta con el c√°lculo de tiempo_entrega_dias y la clasificaci√≥n de entregas como ‚ÄúA tiempo‚Äù o ‚ÄúRetrasadas‚Äù.

Cada job tom√≥ los datos originales desde /raw/, aplic√≥ las transformaciones configuradas y gener√≥ autom√°ticamente un nuevo archivo limpio.

![4](https://github.com/user-attachments/assets/0bb95e84-cb8f-4afa-bb65-032fa611eb76)


**5. Automatizaci√≥n del pipeline**

Una vez validadas y exportadas las transformaciones de los datos, se complet√≥ el desarrollo del pipeline configurando su automatizaci√≥n en la nube, lo que permite a IntraMove mantener un flujo continuo y sin intervenci√≥n manual para procesar sus datos operativos.

Automatizaci√≥n implementada
‚Äã
El flujo fue automatizado mediante la configuraci√≥n de tareas programadas en AWS, lo que garantiza que el sistema procese nuevos archivos de forma peri√≥dica y ordenada. El proceso automatizado sigue los siguientes pasos:

1. Carga peri√≥dica de nuevos archivos a S3
Los nuevos datos de ventas y entregas se cargan regularmente en la carpeta /raw/ del bucket de Amazon S3. Esta ubicaci√≥n funciona como punto de entrada del pipeline, centralizando todos los datos sin procesar.

2. Ejecuci√≥n programada del crawler
‚Äã
Se configur√≥ el crawler de AWS Glue para ejecutarse autom√°ticamente. Este se encarga de:

- Escanear la carpeta /raw/.

- Detectar nuevos archivos incorporados.

- Actualizar las tablas del Glue Data Catalog para mantenerlas sincronizadas con los datos m√°s recientes.


3. Lanzamiento autom√°tico del job de transformaci√≥n
Tras la ejecuci√≥n del crawler, se activa autom√°ticamente el job de AWS Glue DataBrew que aplica las recetas de limpieza y transformaci√≥n a los nuevos datos detectados.

4. Generaci√≥n y almacenamiento del dataset limpio
‚Äã
Una vez transformados, los datos limpios se exportan autom√°ticamente a la carpeta /processed/ del bucket S3, manteniendo la estructura organizada y separada por √°rea (ventas y log√≠stica). Estos archivos est√°n listos para ser consumidos por herramientas de an√°lisis como Power BI o integrarse en otros sistemas de reporting.

![ultsi](https://github.com/user-attachments/assets/ace384cd-13ca-4a21-a04a-4bd3387bd969)


**6. Preparado para escalar**

‚ÄãEl pipeline est√° dise√±ado para crecer y adaptarse a nuevas necesidades. Su arquitectura permite:
‚Äã
- A√±adir nuevas fuentes de datos.

- Incorporar otras capas de validaci√≥n o modelos predictivos.

- Integrarse con consultas SQL en la nube mediante Amazon Athena.

## Reflexiones finales y aprendizajes

Este proyecto ha sido una oportunidad para simular una soluci√≥n de datos completa y aplicable a un entorno empresarial real. La experiencia ha demostrado c√≥mo un flujo de datos bien estructurado y automatizado puede convertirse en un activo estrat√©gico para cualquier empresa.

Desde el primer paso ‚Äîla preparaci√≥n y catalogaci√≥n de los datos‚Äî hasta la automatizaci√≥n del pipeline, el enfoque se ha mantenido firme en resolver una necesidad operativa concreta: transformar de manera automatizada datos dispersos y en bruto en informaci√≥n confiable, actualizada y lista para an√°lisis.

Valor aportado al negocio:
- Reducci√≥n dr√°stica del trabajo manual, al eliminar tareas repetitivas de limpieza y validaci√≥n.

- Agilidad y rapidez en la toma de decisiones, al contar con datos actualizados autom√°ticamente.

- Mayor control log√≠stico, gracias a la capacidad de medir el tiempo de entrega, la puntualidad y el rendimiento de las operaciones.

- Sostenibilidad t√©cnica, al construir una arquitectura escalable en la nube que no depende de programaci√≥n compleja ni recursos t√©cnicos avanzados.‚Äã

‚Äã

Aprendizajes personales

‚Äã
Como analista de datos en formaci√≥n, este proyecto me ha permitido afianzar conocimientos clave relacionados con la ingenier√≠a de datos y la anal√≠tica aplicada al negocio, entre ellos:

- Dise√±ar un flujo ETL completo de forma visual y estructurada.

- Aplicar buenas pr√°cticas de calidad y validaci√≥n del dato.

- Utilizar servicios cloud como S3, Glue y DataBrew de forma coordinada.

- Pensar como analista pero tambi√©n como empresa: c√≥mo ahorrar tiempo, minimizar errores y aumentar el valor de los datos.


## Contacto









‚Äã‚Äã
